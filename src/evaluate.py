"""
Script COMPLETO para avaliar prompts otimizados.

Este script:
1. Carrega dataset de avalia√ß√£o de arquivo .jsonl (datasets/bug_to_user_story.jsonl)
2. Cria/atualiza dataset no LangSmith
3. Puxa prompts otimizados do LangSmith Hub (fonte √∫nica de verdade)
4. Executa prompts contra o dataset
5. Calcula 5 m√©tricas (Helpfulness, Correctness, F1-Score, Clarity, Precision)
6. Publica resultados no dashboard do LangSmith
7. Exibe resumo no terminal

Suporta m√∫ltiplos providers de LLM:
- OpenAI (gpt-4o, gpt-4o-mini)
- Google Gemini (gemini-1.5-flash, gemini-1.5-pro)

Configure o provider no arquivo .env atrav√©s da vari√°vel LLM_PROVIDER.
"""

import os
import sys
import json
import time
from typing import List, Dict, Any
from pathlib import Path
from dotenv import load_dotenv
from langsmith import Client
from langchain import hub
from langchain_core.prompts import ChatPromptTemplate
from utils import check_env_vars, format_score, print_section_header, get_llm as get_configured_llm
from metrics import (
    evaluate_f1_score, evaluate_clarity, evaluate_precision,
    evaluate_tone_score, evaluate_acceptance_criteria_score,
    evaluate_user_story_format_score, evaluate_completeness_score
)

load_dotenv()


def get_llm():
    return get_configured_llm(temperature=0)


def load_dataset_from_jsonl(jsonl_path: str) -> List[Dict[str, Any]]:
    examples = []

    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:  # Ignorar linhas vazias
                    example = json.loads(line)
                    examples.append(example)

        return examples

    except FileNotFoundError:
        print(f"‚ùå Arquivo n√£o encontrado: {jsonl_path}")
        print("\nCertifique-se de que o arquivo datasets/bug_to_user_story.jsonl existe.")
        return []
    except json.JSONDecodeError as e:
        print(f"‚ùå Erro ao parsear JSONL: {e}")
        return []
    except Exception as e:
        print(f"‚ùå Erro ao carregar dataset: {e}")
        return []


def create_evaluation_dataset(client: Client, dataset_name: str, jsonl_path: str) -> str:
    print(f"Criando dataset de avalia√ß√£o: {dataset_name}...")

    examples = load_dataset_from_jsonl(jsonl_path)

    if not examples:
        print("‚ùå Nenhum exemplo carregado do arquivo .jsonl")
        return dataset_name

    print(f"   ‚úì Carregados {len(examples)} exemplos do arquivo {jsonl_path}")

    try:
        datasets = client.list_datasets(dataset_name=dataset_name)
        existing_dataset = None

        for ds in datasets:
            if ds.name == dataset_name:
                existing_dataset = ds
                break

        if existing_dataset:
            print(f"   ‚úì Dataset '{dataset_name}' j√° existe, usando existente")
            return dataset_name
        else:
            dataset = client.create_dataset(dataset_name=dataset_name)

            for example in examples:
                client.create_example(
                    dataset_id=dataset.id,
                    inputs=example["inputs"],
                    outputs=example["outputs"]
                )

            print(f"   ‚úì Dataset criado com {len(examples)} exemplos")
            return dataset_name

    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erro ao criar dataset: {e}")
        return dataset_name


def pull_prompt_from_langsmith(prompt_name: str) -> ChatPromptTemplate:
    try:
        print(f"   Puxando prompt do LangSmith Hub: {prompt_name}")
        prompt = hub.pull(prompt_name)
        print(f"   ‚úì Prompt carregado com sucesso")
        return prompt

    except Exception as e:
        error_msg = str(e).lower()

        print(f"\n{'=' * 70}")
        print(f"‚ùå ERRO: N√£o foi poss√≠vel carregar o prompt '{prompt_name}'")
        print(f"{'=' * 70}\n")

        if "not found" in error_msg or "404" in error_msg:
            print("‚ö†Ô∏è  O prompt n√£o foi encontrado no LangSmith Hub.\n")
            print("A√á√ïES NECESS√ÅRIAS:")
            print("1. Verifique se voc√™ j√° fez push do prompt otimizado:")
            print(f"   python src/push_prompts.py")
            print()
            print("2. Confirme se o prompt foi publicado com sucesso em:")
            print(f"   https://smith.langchain.com/prompts")
            print()
            print(f"3. Certifique-se de que o nome do prompt est√° correto: '{prompt_name}'")
            print()
            print("4. Se voc√™ alterou o prompt no YAML, refa√ßa o push:")
            print(f"   python src/push_prompts.py")
        else:
            print(f"Erro t√©cnico: {e}\n")
            print("Verifique:")
            print("- LANGSMITH_API_KEY est√° configurada corretamente no .env")
            print("- Voc√™ tem acesso ao workspace do LangSmith")
            print("- Sua conex√£o com a internet est√° funcionando")

        print(f"\n{'=' * 70}\n")
        raise


def evaluate_prompt_on_example(
    prompt_template: ChatPromptTemplate,
    example: Any,
    llm: Any
) -> Dict[str, Any]:
    try:
        inputs = example.inputs if hasattr(example, 'inputs') else {}
        outputs = example.outputs if hasattr(example, 'outputs') else {}

        chain = prompt_template | llm

        response = chain.invoke(inputs)
        answer = response.content

        reference = outputs.get("reference", "") if isinstance(outputs, dict) else ""

        if isinstance(inputs, dict):
            question = inputs.get("question", inputs.get("bug_report", inputs.get("pr_title", "N/A")))
        else:
            question = "N/A"

        return {
            "answer": answer,
            "reference": reference,
            "question": question
        }

    except Exception as e:
        print(f"      ‚ö†Ô∏è  Erro ao avaliar exemplo: {e}")
        import traceback
        print(f"      Traceback: {traceback.format_exc()}")
        return {
            "answer": "",
            "reference": "",
            "question": ""
        }


def evaluate_prompt(
    prompt_name: str,
    dataset_name: str,
    client: Client
) -> Dict[str, float]:
    print(f"\nüîç Avaliando: {prompt_name}")

    try:
        prompt_template = pull_prompt_from_langsmith(prompt_name)

        examples = list(client.list_examples(dataset_name=dataset_name))
        print(f"   Dataset: {len(examples)} exemplos")

        llm = get_llm()

        # M√©tricas espec√≠ficas para Bug to User Story
        tone_scores = []
        acceptance_scores = []
        format_scores = []
        completeness_scores = []

        print("   Avaliando exemplos com m√©tricas Bug-to-User-Story...")

        total_examples = min(10, len(examples))
        for i, example in enumerate(examples[:10], 1):
            print(f"      [{i}/{total_examples}] Gerando user story...", flush=True)
            result = evaluate_prompt_on_example(prompt_template, example, llm)
            time.sleep(1)

            if result["answer"]:
                bug_report = result["question"]
                user_story = result["answer"]
                reference = result["reference"]

                print(f"      [{i}/{total_examples}] Avaliando...", end="", flush=True)
                tone = evaluate_tone_score(bug_report, user_story, reference)
                time.sleep(1)
                print(f" T:{tone['score']:.2f}", end="", flush=True)
                acceptance = evaluate_acceptance_criteria_score(bug_report, user_story, reference)
                time.sleep(1)
                print(f" A:{acceptance['score']:.2f}", end="", flush=True)
                fmt = evaluate_user_story_format_score(bug_report, user_story, reference)
                time.sleep(1)
                print(f" F:{fmt['score']:.2f}", end="", flush=True)
                completeness = evaluate_completeness_score(bug_report, user_story, reference)
                time.sleep(1)
                print(f" C:{completeness['score']:.2f}", flush=True)

                tone_scores.append(tone["score"])
                acceptance_scores.append(acceptance["score"])
                format_scores.append(fmt["score"])
                completeness_scores.append(completeness["score"])

                print(f"      [{i}/{total_examples}] Tone:{tone['score']:.2f} Accept:{acceptance['score']:.2f} Format:{fmt['score']:.2f} Complete:{completeness['score']:.2f}")

        avg_tone = sum(tone_scores) / len(tone_scores) if tone_scores else 0.0
        avg_acceptance = sum(acceptance_scores) / len(acceptance_scores) if acceptance_scores else 0.0
        avg_format = sum(format_scores) / len(format_scores) if format_scores else 0.0
        avg_completeness = sum(completeness_scores) / len(completeness_scores) if completeness_scores else 0.0

        return {
            "tone_score": round(avg_tone, 4),
            "acceptance_criteria_score": round(avg_acceptance, 4),
            "user_story_format_score": round(avg_format, 4),
            "completeness_score": round(avg_completeness, 4)
        }

    except Exception as e:
        print(f"   ‚ùå Erro na avalia√ß√£o: {e}")
        return {
            "tone_score": 0.0,
            "acceptance_criteria_score": 0.0,
            "user_story_format_score": 0.0,
            "completeness_score": 0.0
        }


def display_results(prompt_name: str, scores: Dict[str, float]) -> bool:
    print("\n" + "=" * 50)
    print(f"Prompt: {prompt_name}")
    print("=" * 50)

    print("\nM√©tricas Bug-to-User-Story:")
    print(f"  - Tone Score:                {format_score(scores['tone_score'], threshold=0.9)}")
    print(f"  - Acceptance Criteria Score:  {format_score(scores['acceptance_criteria_score'], threshold=0.9)}")
    print(f"  - User Story Format Score:   {format_score(scores['user_story_format_score'], threshold=0.9)}")
    print(f"  - Completeness Score:        {format_score(scores['completeness_score'], threshold=0.9)}")

    average_score = sum(scores.values()) / len(scores)
    all_above_threshold = all(v >= 0.9 for v in scores.values())

    print("\n" + "-" * 50)
    print(f"üìä M√âDIA GERAL: {average_score:.4f}")
    print("-" * 50)

    passed = all_above_threshold and average_score >= 0.9

    if passed:
        print(f"\n‚úÖ STATUS: APROVADO ‚úì - Todas as m√©tricas atingiram o m√≠nimo de 0.9")
    else:
        print(f"\n‚ùå STATUS: REPROVADO - M√©tricas abaixo do m√≠nimo de 0.9")
        print(f"‚ö†Ô∏è  M√©dia atual: {average_score:.4f} | Necess√°rio: 0.9000")
        # Mostrar quais m√©tricas falharam
        for metric, value in scores.items():
            if value < 0.9:
                print(f"   ‚ö†Ô∏è  {metric}: {value:.4f} < 0.9")

    return passed


def main():
    print_section_header("AVALIA√á√ÉO DE PROMPTS OTIMIZADOS")

    provider = os.getenv("LLM_PROVIDER", "openai")
    llm_model = os.getenv("LLM_MODEL", "gpt-4o-mini")
    eval_model = os.getenv("EVAL_MODEL", "gpt-4o")

    print(f"Provider: {provider}")
    print(f"Modelo Principal: {llm_model}")
    print(f"Modelo de Avalia√ß√£o: {eval_model}\n")

    required_vars = ["LANGSMITH_API_KEY", "LLM_PROVIDER"]
    if provider == "openai":
        required_vars.append("OPENAI_API_KEY")
    elif provider in ["google", "gemini"]:
        required_vars.append("GOOGLE_API_KEY")

    if not check_env_vars(required_vars):
        return 1

    client = Client()
    project_name = os.getenv("LANGCHAIN_PROJECT", "prompt-optimization-challenge-resolved")

    jsonl_path = "datasets/bug_to_user_story.jsonl"

    if not Path(jsonl_path).exists():
        print(f"‚ùå Arquivo de dataset n√£o encontrado: {jsonl_path}")
        print("\nCertifique-se de que o arquivo existe antes de continuar.")
        return 1

    dataset_name = f"{project_name}-eval"
    create_evaluation_dataset(client, dataset_name, jsonl_path)

    print("\n" + "=" * 70)
    print("PROMPTS PARA AVALIAR")
    print("=" * 70)
    print("\nEste script ir√° puxar prompts do LangSmith Hub.")
    print("Certifique-se de ter feito push dos prompts antes de avaliar:")
    print("  python src/push_prompts.py\n")

    prompts_to_evaluate = [
        "bug_to_user_story_v2",
    ]

    all_passed = True
    evaluated_count = 0
    results_summary = []

    for prompt_name in prompts_to_evaluate:
        evaluated_count += 1

        try:
            scores = evaluate_prompt(prompt_name, dataset_name, client)

            passed = display_results(prompt_name, scores)
            all_passed = all_passed and passed

            results_summary.append({
                "prompt": prompt_name,
                "scores": scores,
                "passed": passed
            })

        except Exception as e:
            print(f"\n‚ùå Falha ao avaliar '{prompt_name}': {e}")
            all_passed = False

            results_summary.append({
                "prompt": prompt_name,
                "scores": {
                    "tone_score": 0.0,
                    "acceptance_criteria_score": 0.0,
                    "user_story_format_score": 0.0,
                    "completeness_score": 0.0
                },
                "passed": False
            })

    print("\n" + "=" * 50)
    print("RESUMO FINAL")
    print("=" * 50 + "\n")

    if evaluated_count == 0:
        print("‚ö†Ô∏è  Nenhum prompt foi avaliado")
        return 1

    print(f"Prompts avaliados: {evaluated_count}")
    print(f"Aprovados: {sum(1 for r in results_summary if r['passed'])}")
    print(f"Reprovados: {sum(1 for r in results_summary if not r['passed'])}\n")

    if all_passed:
        print("‚úÖ Todos os prompts atingiram m√©dia >= 0.9!")
        print(f"\n‚úì Confira os resultados em:")
        print(f"  https://smith.langchain.com/projects/{project_name}")
        print("\nPr√≥ximos passos:")
        print("1. Documente o processo no README.md")
        print("2. Capture screenshots das avalia√ß√µes")
        print("3. Fa√ßa commit e push para o GitHub")
        return 0
    else:
        print("‚ö†Ô∏è  Alguns prompts n√£o atingiram m√©dia >= 0.9")
        print("\nPr√≥ximos passos:")
        print("1. Refatore os prompts com score baixo")
        print("2. Fa√ßa push novamente: python src/push_prompts.py")
        print("3. Execute: python src/evaluate.py novamente")
        return 1

if __name__ == "__main__":
    sys.exit(main())
